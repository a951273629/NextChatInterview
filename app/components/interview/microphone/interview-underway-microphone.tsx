import React, { useRef, useState, useEffect } from "react";
import StopIcon from "@/app/icons/pause.svg";
import { 
  AzureSpeechRecognizer, 
  getAzureSpeechConfig, 
  isAzureSpeechAvailable 
} from "../azureSpeech";
import styles from "./interview-underway-microphone.module.scss";

// æ¶ˆæ¯ç±»å‹æ¥å£
interface Message {
  id: string;
  text: string;
  isInterviewer: boolean;
  timestamp: number;
}

interface InterviewUnderwayProps {
  // æ§åˆ¶çŠ¶æ€
  visible: boolean;
  voiceprintEnabled: boolean;
  recognitionLanguage: string;

  // å£°çº¹è¯†åˆ«ç›¸å…³
  isInterviewer: boolean;
  voiceMatchScore: number;

  // å›è°ƒå‡½æ•°
  onTextUpdate: (text: string) => void;
  submitMessage: (text: string) => void;
  onStop: () => void;

  // å¯é€‰ï¼šé»˜è®¤è‡ªåŠ¨æäº¤çŠ¶æ€ï¼ˆæ‰¬å£°å™¨æ¨¡å¼ä¸‹é»˜è®¤å¼€å¯ï¼‰
  defaultAutoSubmit?: boolean;

  // ç§»åŠ¨ç«¯ç›¸å…³
  onMinimize?: () => void;
  isMobile?: boolean;

  // æ¶ˆæ¯ç®¡ç†
  messages?: Message[];
  onAddMessage?: (text: string, isInterviewer: boolean) => void;
}

export const InterviewUnderway: React.FC<InterviewUnderwayProps> = ({
  visible,
  voiceprintEnabled,
  recognitionLanguage,
  isInterviewer,
  voiceMatchScore,
  onTextUpdate,
  submitMessage,
  onStop,
  defaultAutoSubmit = false,
  onMinimize,
  isMobile = false,
  messages = [],
  onAddMessage,
}) => {
  // Azure Speech ç›¸å…³çŠ¶æ€
  const [transcript, setTranscript] = useState("");
  const [listening, setListening] = useState(false);
  const [mediaStream, setMediaStream] = useState<MediaStream | null>(null);
  const [recognizer, setRecognizer] = useState<AzureSpeechRecognizer | null>(null);
  const [error, setError] = useState<string | null>(null);
  const [microphoneAvailable, setMicrophoneAvailable] = useState(false);
  const [azureSpeechAvailable, setAzureSpeechAvailable] = useState(false);

  // å¼•ç”¨å˜é‡
  const transcriptRef = useRef(transcript);
  const recognizerRef = useRef<AzureSpeechRecognizer | null>(null);

  // æ§åˆ¶çŠ¶æ€
  const [isPaused, setIsPaused] = useState(false);
  const [isAutoSubmit, setIsAutoSubmit] = useState(defaultAutoSubmit);
  const [showTooltip, setShowTooltip] = useState(true);

  // æ¶ˆæ¯ç›¸å…³ - ç§»é™¤å†…éƒ¨çŠ¶æ€ï¼Œä½¿ç”¨å¤–éƒ¨ä¼ å…¥çš„
  // const [messages, setMessages] = useState<Message[]>([]);
  const messagesEndRef = useRef<HTMLDivElement>(null);
  const lastSubmittedTextRef = useRef("");

  // å°†transcriptæ›´æ–°åˆ°çˆ¶ç»„ä»¶
  useEffect(() => {
    transcriptRef.current = transcript;
    onTextUpdate(transcript);
  }, [transcript, onTextUpdate]);

  // æ»šåŠ¨åˆ°æœ€æ–°æ¶ˆæ¯
  const scrollToBottom = () => {
    messagesEndRef.current?.scrollIntoView({ behavior: "smooth" });
  };

  // æ¯å½“æ¶ˆæ¯åˆ—è¡¨æ›´æ–°æ—¶ï¼Œæ»šåŠ¨åˆ°åº•éƒ¨
  useEffect(() => {
    scrollToBottom();
  }, [messages]);

  // æ¶ˆæ¯æ·»åŠ å‡½æ•° - ä½¿ç”¨å¤–éƒ¨ä¼ å…¥çš„å›è°ƒ
  // const addMessage = (text: string, isInterviewer: boolean) => {
  //   if (onAddMessage) {
  //     onAddMessage(text, isInterviewer);
  //   }
  // };

  // æ¶ˆæ¯ç‚¹å‡»å¤„ç†å‡½æ•°
  const handleMessageClick = (messageText: string) => {
    console.log("æ¶ˆæ¯è¢«ç‚¹å‡»:", messageText);
    submitMessage(messageText);
    
    // åœ¨ç§»åŠ¨ç«¯æ¨¡å¼ä¸‹ï¼Œç‚¹å‡»æ¶ˆæ¯åæœ€å°åŒ–é¡µé¢
    if (isMobile && onMinimize) {
      onMinimize();
    }
  };

  // æ£€æŸ¥ Azure Speech å¯ç”¨æ€§
  useEffect(() => {
    const checkAvailability = async () => {
      try {
        const available = isAzureSpeechAvailable();
        setAzureSpeechAvailable(available);
        console.log("ğŸ” Azure Speech å¯ç”¨æ€§æ£€æŸ¥:", available);
      } catch (error) {
        console.error("âŒ Azure Speech å¯ç”¨æ€§æ£€æŸ¥å¤±è´¥:", error);
        setError("Azure Speech æœåŠ¡ä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥é…ç½®");
        setAzureSpeechAvailable(false);
      }
    };

    checkAvailability();
  }, []);

  // è¯·æ±‚éº¦å…‹é£æƒé™
  const requestMicrophonePermission = async (): Promise<MediaStream | null> => {
    try {
      console.log("ğŸ¤ è¯·æ±‚éº¦å…‹é£æƒé™...");
      
      if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
        throw new Error("æµè§ˆå™¨ä¸æ”¯æŒéº¦å…‹é£è®¿é—®");
      }

      const stream = await navigator.mediaDevices.getUserMedia({
        audio: {
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true,
          sampleRate: 16000,
        }
      });

      console.log("âœ… éº¦å…‹é£æƒé™è·å–æˆåŠŸ");
      setMicrophoneAvailable(true);
      setError(null);
      return stream;
    } catch (error) {
      console.error("âŒ éº¦å…‹é£æƒé™è·å–å¤±è´¥:", error);
      setMicrophoneAvailable(false);
      
      if (error instanceof Error) {
        if (error.name === 'NotAllowedError') {
          setError("éº¦å…‹é£æƒé™è¢«æ‹’ç»ï¼Œè¯·å…è®¸éº¦å…‹é£è®¿é—®");
        } else if (error.name === 'NotFoundError') {
          setError("æœªæ‰¾åˆ°éº¦å…‹é£è®¾å¤‡");
        } else {
          setError(`éº¦å…‹é£è®¿é—®å¤±è´¥: ${error.message}`);
        }
      } else {
        setError("éº¦å…‹é£è®¿é—®å¤±è´¥");
      }
      return null;
    }
  };

  // åˆå§‹åŒ– Azure Speech è¯†åˆ«å™¨
  const initializeRecognizer = async (): Promise<AzureSpeechRecognizer | null> => {
    try {
      console.log("ğŸš€ åˆå§‹åŒ– Azure Speech è¯†åˆ«å™¨...");
      
      if (!azureSpeechAvailable) {
        throw new Error("Azure Speech æœåŠ¡ä¸å¯ç”¨");
      }

      // è·å–é…ç½®
      const config = getAzureSpeechConfig();
      config.language = recognitionLanguage || "zh-CN";

      // åˆ›å»ºè¯†åˆ«å™¨
      const newRecognizer = new AzureSpeechRecognizer(config);

      // è·å–éº¦å…‹é£æµ
      const stream = await requestMicrophonePermission();
      if (!stream) {
        throw new Error("æ— æ³•è·å–éº¦å…‹é£éŸ³é¢‘æµ");
      }

      // è®¾ç½®éŸ³é¢‘é…ç½®
      newRecognizer.createAudioConfigFromStream(stream);

      setMediaStream(stream);
      setRecognizer(newRecognizer);
      recognizerRef.current = newRecognizer;

      console.log("âœ… Azure Speech è¯†åˆ«å™¨åˆå§‹åŒ–æˆåŠŸ");
      return newRecognizer;
    } catch (error) {
      console.error("âŒ åˆå§‹åŒ– Azure Speech è¯†åˆ«å™¨å¤±è´¥:", error);
      setError(typeof error === "string" ? error : (error as Error).message);
      return null;
    }
  };

  // å¤„ç†è¯†åˆ«ç»“æœ
  const handleRecognitionResult = (text: string, isFinal: boolean) => {
    console.log(`ğŸ”„ è¯†åˆ«ç»“æœ (${isFinal ? 'æœ€ç»ˆ' : 'ä¸´æ—¶'}):`, text);
    
    // æ€»æ˜¯æ›´æ–°å½“å‰æ˜¾ç¤ºçš„æ–‡æœ¬
    setTranscript(text);
    transcriptRef.current = text;
    onTextUpdate(text);
    
    // åªæœ‰æœ€ç»ˆç»“æœæ‰è§¦å‘è‡ªåŠ¨æäº¤ç›¸å…³é€»è¾‘ï¼Œä¸éœ€è¦å»¶è¿Ÿ
    if (isFinal && text && text.trim() !== "") {
      console.log("ğŸ¯ æœ€ç»ˆè¯†åˆ«ç»“æœï¼Œç«‹å³å¤„ç†è‡ªåŠ¨æäº¤é€»è¾‘:", text);
      
      // ç›´æ¥å¤„ç†è‡ªåŠ¨æäº¤é€»è¾‘ï¼Œä¸éœ€è¦å»¶è¿Ÿ
      // å¦‚æœå£°çº¹è¯†åˆ«å¯ç”¨ï¼Œå¹¶ä¸”è¢«è¯†åˆ«ä¸ºé¢è¯•å®˜
      if (voiceprintEnabled && isInterviewer) {
        console.log("æ£€æµ‹åˆ°é¢è¯•å®˜è¯­éŸ³ï¼Œæ·»åŠ åˆ°æ¶ˆæ¯å†å²:", text);
        onAddMessage?.(text, true);
        lastSubmittedTextRef.current = text;
        // é‡ç½®æ–‡æœ¬
        setTranscript("");
        transcriptRef.current = "";
        onTextUpdate("");
      }
      // å¦‚æœæ˜¯é¢è¯•è€…æˆ–å£°çº¹æœªå¯ç”¨
      else if (text !== lastSubmittedTextRef.current) {
        console.log("æ£€æµ‹åˆ°é¢è¯•è€…è¯­éŸ³ï¼Œæ·»åŠ åˆ°æ¶ˆæ¯å†å²:", text);
        onAddMessage?.(text, false);
        lastSubmittedTextRef.current = text;
        // é‡ç½®æ–‡æœ¬
        setTranscript("");
        transcriptRef.current = "";
        onTextUpdate("");
      }

      // å¦‚æœè‡ªåŠ¨æäº¤å¼€å¯
      if (isAutoSubmit && (isInterviewer || !voiceprintEnabled)) {
        console.log("è‡ªåŠ¨æäº¤é¢è¯•è€…è¯­éŸ³:", text);
        submitMessage(text);
      }
    }
  };

  // å¤„ç†è¯†åˆ«é”™è¯¯
  const handleRecognitionError = (errorMessage: string) => {
    console.error("âŒ è¯­éŸ³è¯†åˆ«é”™è¯¯:", errorMessage);
    setError(errorMessage);
    setListening(false);
  };

  // å¤„ç†è¯†åˆ«ç»“æŸ
  const handleRecognitionEnd = () => {
    console.log("ğŸ è¯­éŸ³è¯†åˆ«ç»“æŸ");
    setListening(false);
  };

  // å¼€å§‹è¯­éŸ³è¯†åˆ«
  const startListening = async () => {
    try {
      console.log("â–¶ï¸ å¼€å§‹è¯­éŸ³è¯†åˆ«...");
      
      let currentRecognizer = recognizerRef.current;
      
      if (!currentRecognizer) {
        currentRecognizer = await initializeRecognizer();
        if (!currentRecognizer) {
          throw new Error("è¯†åˆ«å™¨åˆå§‹åŒ–å¤±è´¥");
        }
      }

      // å¼€å§‹è¿ç»­è¯†åˆ«
      currentRecognizer.startContinuousRecognition(
        handleRecognitionResult,
        handleRecognitionError,
        handleRecognitionEnd
      );

      setListening(true);
      setError(null);
      
      console.log("âœ… è¯­éŸ³è¯†åˆ«å·²å¯åŠ¨");
    } catch (error) {
      console.error("âŒ å¯åŠ¨è¯­éŸ³è¯†åˆ«å¤±è´¥:", error);
      setError(typeof error === "string" ? error : (error as Error).message);
      setListening(false);
    }
  };

  // åœæ­¢è¯­éŸ³è¯†åˆ«
  const stopListening = () => {
    console.log("â¹ï¸ åœæ­¢è¯­éŸ³è¯†åˆ«...");
    
    if (recognizerRef.current) {
      recognizerRef.current.stopRecognition();
    }
    
    setListening(false);
  };

  // é‡ç½®è¯†åˆ«æ–‡æœ¬
  const resetTranscript = () => {
    console.log("ğŸ—‘ï¸ é‡ç½®è¯†åˆ«æ–‡æœ¬");
    setTranscript("");
    transcriptRef.current = "";
    onTextUpdate("");
  };

  // æ¸…ç†èµ„æº
  const cleanup = () => {
    console.log("ğŸ§¹ æ¸…ç† Azure Speech èµ„æº...");
    
    // åœæ­¢è¯†åˆ«
    if (recognizerRef.current) {
      recognizerRef.current.dispose();
      recognizerRef.current = null;
    }

    // å…³é—­åª’ä½“æµ
    if (mediaStream) {
      mediaStream.getTracks().forEach(track => {
        track.stop();
        console.log("ğŸ”‡ éŸ³é¢‘è½¨é“å·²åœæ­¢:", track.label);
      });
      setMediaStream(null);
    }

    setRecognizer(null);
    setListening(false);
    setError(null);
  };

  // å½“ç»„ä»¶å¯è§ä¸”æœªæš‚åœæ—¶å¼€å§‹è¯­éŸ³è¯†åˆ«
  useEffect(() => {
    if (visible && !isPaused && azureSpeechAvailable) {
      startListening();
    } else if (!visible || isPaused) {
      stopListening();
    }

    return () => {
      if (!visible) {
        cleanup();
      }
    };
  }, [visible, isPaused, azureSpeechAvailable, recognitionLanguage]);

  // æš‚åœ/æ¢å¤åŠŸèƒ½
  const togglePauseCommit = () => {
    if (!isPaused) {
      stopListening();
      resetTranscript();
    } else if (azureSpeechAvailable) {
      startListening();
    }
    setIsPaused(!isPaused);
  };

  // åœæ­¢è¯†åˆ«
  const stopRecognition = () => {
    try {
      cleanup();
      onStop();
    } catch (error) {
      console.error("åœæ­¢è¯­éŸ³è¯†åˆ«å¤±è´¥:", error);
    }
  };

  // ç»„ä»¶å¸è½½æ—¶æ¸…ç†
  useEffect(() => {
    return () => {
      // æ¸…ç† Azure Speech èµ„æº
      cleanup();
    };
  }, []);

  return (
    <>
      {/* è¯­éŸ³è¯†åˆ«çŠ¶æ€æŒ‡ç¤ºå™¨ */}
      <div className={styles.statusIndicator}>
        <div
          className={`${styles.indicatorDot} ${
            listening ? styles.listening : styles.notListening
          }`}
        />
        <span className={styles.statusText}>
          {listening ? "æ­£åœ¨ç›‘å¬..." : isPaused ? "å·²æš‚åœ" : "æœªç›‘å¬"}
        </span>

        {/* æ·»åŠ å¯ç‚¹å‡»æç¤ºæ°”æ³¡ */}
        {showTooltip && (
          <div className={styles.clickableTooltip}>
            <div className={styles.tooltipContent}>å•å‡»ä¸‹é¢çš„æ¶ˆæ¯è·å–å›ç­”</div>
            <button
              className={styles.tooltipCloseBtn}
              onClick={() => setShowTooltip(false)}
            >
              âœ•
            </button>
          </div>
        )}

        {/* æ·»åŠ å£°çº¹è¯†åˆ«çŠ¶æ€æ˜¾ç¤º */}
        {voiceprintEnabled && (
          <div className={styles.voiceprintStatus}>
            <span
              className={`${styles.identityIndicator} ${
                isInterviewer ? styles.interviewer : styles.interviewee
              }`}
            >
              {isInterviewer ? "é¢è¯•å®˜" : "é¢è¯•è€…"}
            </span>
            {voiceMatchScore > 0 && (
              <span className={styles.matchScore}>
                ç›¸ä¼¼åº¦: {(voiceMatchScore * 100).toFixed(1)}%
              </span>
            )}
          </div>
        )}
      </div>

      {/* é”™è¯¯æç¤º */}
      {(!azureSpeechAvailable || !microphoneAvailable || error) && (
        <div className={styles.errorMessage}>
          {!azureSpeechAvailable
            ? "Azure Speech æœåŠ¡ä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥é…ç½®"
            : !microphoneAvailable
            ? "æ— æ³•è®¿é—®éº¦å…‹é£ï¼Œè¯·æ£€æŸ¥éº¦å…‹é£æƒé™"
            : error}
        </div>
      )}

      {/* æ¶ˆæ¯å†å²è®°å½•åŒºåŸŸ */}
      <div className={styles.messagesContainer}>
        {messages.map((message) => (
          <div
            key={message.id}
            className={`${styles.message} ${
              message.isInterviewer
                ? styles.interviewerMessage
                : styles.intervieweeMessage
            }`}
            onClick={() => handleMessageClick(message.text)}
          >
            {message.text}
          </div>
        ))}
        <div ref={messagesEndRef} />
      </div>

      {/* å½“å‰è¯†åˆ«æ–‡æœ¬æ˜¾ç¤ºåŒºåŸŸ */}
      {transcript && transcript.trim() !== "" && (
        <div
          className={`${styles.transcriptDisplay} ${
            voiceprintEnabled && isInterviewer
              ? styles.interviewerText
              : styles.intervieweeText
          }`}
        >
          <div className={styles.transcriptLabel}>å½“å‰è¯†åˆ«:</div>
          {transcript}
        </div>
      )}

      {/* æŒ‰é’®åŒºåŸŸ */}
      <div className={styles.buttonContainer}>
        {/* æ·»åŠ è‡ªåŠ¨æäº¤çš„å¼€å…³ */}
        <div className={styles.settingItem}>
          <div className={styles.settingLabel}>è‡ªåŠ¨æäº¤ï¼š</div>
          <div className={styles.settingControl}>
            <label className={styles.switch}>
              <input
                type="checkbox"
                checked={isAutoSubmit}
                onChange={
                  voiceprintEnabled
                    ? () => setIsAutoSubmit(!isAutoSubmit)
                    : () => {}
                }
              />
              <span className={styles.slider}></span>
            </label>
            <span className={styles.settingStatus}>
              {voiceprintEnabled ? "å¯å¯ç”¨" : "å£°çº¹æœªå¯ç”¨,è¯·å…ˆæ‰“å¼€å£°çº¹è¯†åˆ«"}
            </span>
          </div>
        </div>

        {/* æš‚åœæ¢å¤æŒ‰é’® */}
        <button
          onClick={togglePauseCommit}
          className={`${styles.button} ${styles.pauseButton} ${
            isPaused ? styles.paused : ""
          }`}
        >
          <span>{isPaused ? "â–¶ï¸ æ¢å¤ç›‘å¬" : "â¸ï¸ æš‚åœç›‘å¬"}</span>
        </button>

        <button
          onClick={stopRecognition}
          className={`${styles.button} ${styles.stopButton}`}
        >
          <StopIcon />
          <span>ç»“æŸé¢è¯•</span>
        </button>

        <button
          onClick={resetTranscript}
          className={`${styles.button} ${styles.clearButton}`}
        >
          <span>ğŸ—‘ï¸ æ¸…ç©º</span>
        </button>
      </div>
    </>
  );
};

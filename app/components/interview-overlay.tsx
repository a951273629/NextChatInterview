import React, { useState, useEffect, useRef } from "react";
import StopIcon from "../icons/pause.svg";
import SpeechRecognition, {
  useSpeechRecognition,
} from "react-speech-recognition";
import "./interview-overlay.scss";
import * as tf from "@tensorflow/tfjs";
import {
  RealtimeVoiceprintRecognizer,
  VoiceRecognitionStatus,
  loadVoiceprintModelAndRecognizer,
} from "../services/voiceprint-service";
import InterviewPreparation from "./InterviewPreparation";
import { Toaster } from "react-hot-toast";

// æ¶ˆæ¯ç±»å‹æ¥å£
interface Message {
  id: string;
  text: string;
  isInterviewer: boolean;
  timestamp: number;
}

interface InterviewOverlayProps {
  onClose: () => void;
  onTextUpdate: (text: string) => void;
  submitMessage: (text: string) => void;
}

export const InterviewOverlay: React.FC<InterviewOverlayProps> = ({
  onClose,
  onTextUpdate,
  submitMessage,
}) => {
  const [visible, setVisible] = useState(true);
  const [isPaused, setIsPaused] = useState(false);
  const [width, setWidth] = useState("33vw");
  const [isDragging, setIsDragging] = useState(false);
  const isDraggingRef = useRef(isDragging);
  const dragStartXRef = useRef(0);
  const initialWidthRef = useRef(0);

  // æ·»åŠ æ§åˆ¶é¢è¯•å¼€å§‹çš„çŠ¶æ€
  const [isStarted, setIsStarted] = useState(false);

  // æ·»åŠ è‡ªåŠ¨æäº¤æ§åˆ¶çŠ¶æ€
  const [isAutoSubmit, setIsAutoSubmit] = useState(false);
  // æ·»åŠ æ°”æ³¡æç¤ºæ˜¾ç¤ºæ§åˆ¶
  const [showTooltip, setShowTooltip] = useState(true);

  // å£°çº¹è¯†åˆ«ç›¸å…³çŠ¶æ€
  const [voiceprintEnabled, setVoiceprintEnabled] = useState(true);
  const [isInterviewer, setIsInterviewer] = useState(false);
  const [voiceMatchScore, setVoiceMatchScore] = useState(0);
  const [recognitionStatus, setRecognitionStatus] =
    useState<VoiceRecognitionStatus>(VoiceRecognitionStatus.IDLE);

  // æ·»åŠ æ¶ˆæ¯å†å²è®°å½•çŠ¶æ€
  const [messages, setMessages] = useState<Message[]>([]);
  const messagesEndRef = useRef<HTMLDivElement>(null);

  // æ·»åŠ è¯­è¨€é€‰æ‹©çŠ¶æ€ - ä»localStorageåˆå§‹åŒ–
  const [recognitionLanguage, setRecognitionLanguage] = useState<string>(
    localStorage.getItem("interviewLanguage") || "zh-CN",
  );

  // å£°çº¹è¯†åˆ«å™¨å¼•ç”¨
  const recognizerRef = useRef<RealtimeVoiceprintRecognizer | null>(null);
  const modelRef = useRef<tf.LayersModel | null>(null);
  const voiceprintRef = useRef<Float32Array | null>(null);

  // å…¶ä»–å¿…è¦å¼•ç”¨
  const lastSubmittedTextRef = useRef("");
  const isInterviewerRef = useRef(isInterviewer);
  const voiceMatchScoreRef = useRef(voiceMatchScore);
  const audioContextRef = useRef<AudioContext | null>(null);
  const audioStreamRef = useRef<MediaStream | null>(null);
  const audioProcessorRef = useRef<ScriptProcessorNode | null>(null);
  const collectedAudioDataRef = useRef<Float32Array[]>([]);
  const [isCollectingAudio, setIsCollectingAudio] = useState(false);
  const autoSubmitTimerRef = useRef<NodeJS.Timeout | null>(null);

  // è¯­éŸ³è¯†åˆ«ç›¸å…³
  const {
    transcript,
    listening,
    resetTranscript,
    browserSupportsSpeechRecognition,
    isMicrophoneAvailable,
  } = useSpeechRecognition();
  const transcriptRef = useRef(transcript);

  // æ»šåŠ¨åˆ°æœ€æ–°æ¶ˆæ¯
  const scrollToBottom = () => {
    messagesEndRef.current?.scrollIntoView({ behavior: "smooth" });
  };

  // æ¶ˆæ¯æ·»åŠ å‡½æ•°
  const addMessage = (text: string, isInterviewer: boolean) => {
    if (!text || text.trim() === "") return;

    const newMessage: Message = {
      id: `msg-${Date.now()}-${Math.random().toString(36).substring(2, 9)}`,
      text: text.trim(),
      isInterviewer,
      timestamp: Date.now(),
    };

    setMessages((prev) => [...prev, newMessage]);
    // åœ¨æ¶ˆæ¯æ·»åŠ åæ»šåŠ¨åˆ°åº•éƒ¨
    setTimeout(scrollToBottom, 100);
  };

  // æ¶ˆæ¯ç‚¹å‡»å¤„ç†å‡½æ•°
  const handleMessageClick = (messageText: string) => {
    console.log("æ¶ˆæ¯è¢«ç‚¹å‡»:", messageText);
    submitMessage(messageText);
  };

  // åŠ è½½TensorFlowæ¨¡å‹å’Œå£°çº¹ç‰¹å¾
  useEffect(() => {
    const setupVoiceprintSystem = async () => {
      try {
        // ä½¿ç”¨æå–åˆ°æœåŠ¡ä¸­çš„å‡½æ•°åŠ è½½æ¨¡å‹å’Œå£°çº¹
        const result = await loadVoiceprintModelAndRecognizer(
          setRecognitionStatus,
          setVoiceprintEnabled,
          handleVoiceprintResult,
        );

        // æ›´æ–°å¼•ç”¨
        modelRef.current = result.model;
        voiceprintRef.current = result.voiceprint;
        recognizerRef.current = result.recognizer;
      } catch (error) {
        console.error("å£°çº¹ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥:", error);
        setRecognitionStatus(VoiceRecognitionStatus.ERROR);
        setVoiceprintEnabled(false);
      }
    };

    setupVoiceprintSystem();

    // ç»„ä»¶å¸è½½æ—¶æ¸…ç†èµ„æº
    return () => {
      console.log("InterviewOverlayç»„ä»¶å¸è½½ï¼Œæ¸…ç†æ‰€æœ‰èµ„æº");

      // åœæ­¢è¯­éŸ³è¯†åˆ«
      try {
        SpeechRecognition.abortListening();
        SpeechRecognition.stopListening();
      } catch (e) {
        console.error("åœæ­¢è¯­éŸ³è¯†åˆ«å¤±è´¥:", e);
      }

      // åœæ­¢éŸ³é¢‘é‡‡é›†
      stopAudioCollection();

      // æ¸…ç†å£°çº¹è¯†åˆ«å™¨
      if (recognizerRef.current) {
        try {
          recognizerRef.current.clearBuffer();
          recognizerRef.current = null;
        } catch (e) {
          console.error("æ¸…ç†å£°çº¹è¯†åˆ«å™¨å¤±è´¥:", e);
        }
      }

      // é‡Šæ”¾TensorFlowæ¨¡å‹
      if (modelRef.current) {
        try {
          modelRef.current.dispose();
          console.log("å¸è½½æ—¶æ¨¡å‹å·²é”€æ¯");
          modelRef.current = null;
        } catch (e) {
          console.error("æ¨¡å‹é”€æ¯å‡ºé”™:", e);
        }
      }

      // æœ€åä¸€æ¬¡ç¡®ä¿æ‰€æœ‰éŸ³é¢‘è½¨é“éƒ½è¢«åœæ­¢
      if (audioStreamRef.current) {
        try {
          const tracks = audioStreamRef.current.getTracks();
          tracks.forEach((track) => {
            if (track.readyState === "live") {
              track.stop();
            }
          });
          audioStreamRef.current = null;
        } catch (e) {
          console.error("åœæ­¢éŸ³é¢‘è½¨é“å¤±è´¥:", e);
        }
      }

      // æ¸…é™¤è‡ªåŠ¨æäº¤è®¡æ—¶å™¨
      if (autoSubmitTimerRef.current) {
        clearTimeout(autoSubmitTimerRef.current);
        autoSubmitTimerRef.current = null;
      }
    };
  }, []);

  // å¤„ç†å£°çº¹è¯†åˆ«ç»“æœ
  const handleVoiceprintResult = (result: {
    isMatch: boolean;
    score: number;
  }) => {
    setVoiceMatchScore(result.score);

    // ä¿®æ”¹ï¼šåè½¬é€»è¾‘ï¼ŒåŒ¹é…æˆåŠŸ(isMatch=true)è¡¨ç¤ºæ˜¯é¢è¯•è€…ï¼Œè€Œä¸æ˜¯é¢è¯•å®˜
    setIsInterviewer(!result.isMatch);

    // æ›´æ–°å¼•ç”¨å€¼ï¼Œç¡®ä¿effectå¤–éƒ¨è®¿é—®æœ€æ–°çŠ¶æ€
    isInterviewerRef.current = !result.isMatch; // åŒæ ·éœ€è¦åè½¬
    voiceMatchScoreRef.current = result.score;

    // æ›´æ–°è¯†åˆ«çŠ¶æ€ï¼ˆä¿æŒä¸å˜ï¼Œå› ä¸ºè¿™åªä»£è¡¨åŒ¹é…çŠ¶æ€ï¼Œä¸ä»£è¡¨èº«ä»½ï¼‰
    setRecognitionStatus(
      result.isMatch
        ? VoiceRecognitionStatus.MATCHED
        : VoiceRecognitionStatus.NOT_MATCHED,
    );

    // ä¿®æ”¹æ—¥å¿—ä¿¡æ¯ï¼ŒåŒ¹é…æˆåŠŸæ˜¾ç¤ºä¸ºé¢è¯•è€…
    console.log(
      `å£°çº¹è¯†åˆ«ç»“æœ: ${!result.isMatch ? "é¢è¯•å®˜" : "é¢è¯•è€…"}, ç›¸ä¼¼åº¦: ${(
        result.score * 100
      ).toFixed(2)}%`,
    );
  };

  // å°†transcriptæ›´æ–°åˆ°çˆ¶ç»„ä»¶
  useEffect(() => {
    transcriptRef.current = transcript;
    onTextUpdate(transcript);
  }, [transcript, onTextUpdate]);

  // æ¯å½“æ¶ˆæ¯åˆ—è¡¨æ›´æ–°æ—¶ï¼Œæ»šåŠ¨åˆ°åº•éƒ¨
  useEffect(() => {
    scrollToBottom();
  }, [messages]);

  // å¼€å§‹éŸ³é¢‘æ”¶é›†å¹¶å¤„ç†
  const startAudioCollection = async () => {
    if (isCollectingAudio) return;

    try {
      setIsCollectingAudio(true);
      collectedAudioDataRef.current = [];

      // è¯·æ±‚éº¦å…‹é£æƒé™
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      audioStreamRef.current = stream;

      // åˆ›å»ºéŸ³é¢‘ä¸Šä¸‹æ–‡
      const audioContext = new (window.AudioContext ||
        (window as any).webkitAudioContext)();
      audioContextRef.current = audioContext;

      // åˆ›å»ºåˆ†æå™¨èŠ‚ç‚¹
      const analyser = audioContext.createAnalyser();
      analyser.fftSize = 1024;

      // åˆ›å»ºéŸ³é¢‘æº
      const source = audioContext.createMediaStreamSource(stream);
      source.connect(analyser);

      // åˆ›å»ºå¤„ç†å™¨èŠ‚ç‚¹
      const processor = audioContext.createScriptProcessor(4096, 1, 1);
      audioProcessorRef.current = processor;

      // å¤„ç†éŸ³é¢‘æ•°æ®
      processor.onaudioprocess = (e) => {
        const inputData = e.inputBuffer.getChannelData(0);
        const audioData = new Float32Array(inputData);
        collectedAudioDataRef.current.push(audioData);

        // å°†æ•°æ®å‘é€ç»™å®æ—¶è¯†åˆ«å™¨
        if (voiceprintEnabled && recognizerRef.current) {
          recognizerRef.current.addAudioData(audioData);
        }
      };

      // è¿æ¥èŠ‚ç‚¹
      analyser.connect(processor);
      processor.connect(audioContext.destination);
    } catch (error) {
      console.error("å¼€å§‹éŸ³é¢‘é‡‡é›†å¤±è´¥:", error);
      setIsCollectingAudio(false);
    }
  };

  // åœæ­¢éŸ³é¢‘é‡‡é›†
  const stopAudioCollection = () => {
    try {
      // ç¡®ä¿å¤„ç†å™¨æ–­å¼€è¿æ¥
      if (audioProcessorRef.current && audioContextRef.current) {
        try {
          audioProcessorRef.current.disconnect();
        } catch (e) {
          console.error("æ–­å¼€å¤„ç†å™¨è¿æ¥å¤±è´¥:", e);
        }
        audioProcessorRef.current = null;
      }

      // ç¡®ä¿éŸ³é¢‘ä¸Šä¸‹æ–‡å…³é—­
      if (audioContextRef.current) {
        try {
          // æ£€æŸ¥éŸ³é¢‘ä¸Šä¸‹æ–‡çŠ¶æ€
          if (audioContextRef.current.state !== "closed") {
            audioContextRef.current
              .close()
              .catch((err) => console.error("å…³é—­éŸ³é¢‘ä¸Šä¸‹æ–‡å¤±è´¥:", err));
          }
        } catch (e) {
          console.error("éŸ³é¢‘ä¸Šä¸‹æ–‡å…³é—­å¼‚å¸¸:", e);
        }
        audioContextRef.current = null;
      }

      // ç¡®ä¿æ‰€æœ‰éŸ³é¢‘è½¨é“éƒ½è¢«åœæ­¢
      if (audioStreamRef.current) {
        try {
          const tracks = audioStreamRef.current.getTracks();
          tracks.forEach((track) => {
            try {
              if (track.readyState === "live") {
                track.stop();
                console.log("æˆåŠŸåœæ­¢éŸ³é¢‘è½¨é“");
              }
            } catch (e) {
              console.error("åœæ­¢éŸ³é¢‘è½¨é“å¤±è´¥:", e);
            }
          });
          // å¼ºåˆ¶è®¾ç½®ä¸ºnullä»¥å¸®åŠ©åƒåœ¾å›æ”¶
          audioStreamRef.current = null;
        } catch (e) {
          console.error("åœæ­¢éŸ³é¢‘æµå¤±è´¥:", e);
        }
      }

      // æ¸…ç©ºæ”¶é›†çš„éŸ³é¢‘æ•°æ®
      collectedAudioDataRef.current = [];
      setIsCollectingAudio(false);

      // ä¸ºç¡®ä¿æ‰€æœ‰èµ„æºè¢«é‡Šæ”¾ï¼Œæ˜¾å¼è¯·æ±‚åƒåœ¾å›æ”¶
      if (window.gc) {
        try {
          window.gc();
        } catch (e) {}
      }
    } catch (error) {
      console.error("åœæ­¢éŸ³é¢‘é‡‡é›†å®Œå…¨å¤±è´¥:", error);
    }
  };

  // å½“ç»„ä»¶å¯è§ä¸”æœªæš‚åœä¸”å·²å¼€å§‹é¢è¯•æ—¶å¼€å§‹è¯­éŸ³è¯†åˆ«
  useEffect(() => {
    if (visible && !isPaused && isStarted) {
      // ä»localStorageè·å–è¯­è¨€è®¾ç½®
      const savedLanguage =
        localStorage.getItem("interviewLanguage") || "zh-CN";

      // é…ç½®è¯­éŸ³è¯†åˆ«
      SpeechRecognition.startListening({
        continuous: true,
        language: savedLanguage,
      });

      // å¼€å§‹éŸ³é¢‘é‡‡é›†å’Œå£°çº¹è¯†åˆ«
      if (voiceprintEnabled) {
        startAudioCollection();
      }
    }

    return () => {
      SpeechRecognition.stopListening();
      stopAudioCollection();
    };
  }, [visible, isPaused, voiceprintEnabled, isStarted]);

  // è‡ªåŠ¨æäº¤é¢è¯•å®˜è¯­éŸ³
  useEffect(() => {
    // æ¸…é™¤ä¹‹å‰çš„è®¡æ—¶å™¨
    if (autoSubmitTimerRef.current) {
      clearTimeout(autoSubmitTimerRef.current);
      autoSubmitTimerRef.current = null;
    }

    // å¦‚æœæœ‰æ–‡æœ¬å†…å®¹
    if (transcript && transcript.trim() !== "") {
      // è®¾ç½®ä¸€ä¸ªçŸ­æš‚çš„å»¶è¿Ÿï¼Œç¡®ä¿æ”¶é›†åˆ°å®Œæ•´çš„å¥å­
      autoSubmitTimerRef.current = setTimeout(() => {
        // åªæœ‰å½“transcriptæ²¡æœ‰å˜åŒ–æ—¶æ‰å¤„ç†ï¼Œé¿å…å¥å­è¿˜åœ¨å½¢æˆè¿‡ç¨‹ä¸­å°±å¤„ç†
        if (transcript === transcriptRef.current) {
          // å¦‚æœå£°çº¹è¯†åˆ«å¯ç”¨ï¼Œå¹¶ä¸”è¢«è¯†åˆ«ä¸ºé¢è¯•å®˜
          if (voiceprintEnabled && isInterviewerRef.current) {
            console.log("æ£€æµ‹åˆ°é¢è¯•å®˜è¯­éŸ³ï¼Œæ·»åŠ åˆ°æ¶ˆæ¯å†å²:", transcript);
            // æ·»åŠ åˆ°æ¶ˆæ¯å†å²
            addMessage(transcript, true);
            lastSubmittedTextRef.current = transcript;
            resetTranscript();
          }
          // å¦‚æœæ˜¯é¢è¯•è€…æˆ–å£°çº¹æœªå¯ç”¨
          else if (transcript !== lastSubmittedTextRef.current) {
            console.log("æ£€æµ‹åˆ°é¢è¯•è€…è¯­éŸ³ï¼Œæ·»åŠ åˆ°æ¶ˆæ¯å†å²:", transcript);
            // æ·»åŠ åˆ°æ¶ˆæ¯å†å²
            addMessage(transcript, false);
            lastSubmittedTextRef.current = transcript;
            resetTranscript();
          }

          // submitMessage
          // å¦‚æœè‡ªåŠ¨æäº¤å¼€å¯
          if (
            isAutoSubmit &&
            (isInterviewerRef.current || !voiceprintEnabled)
          ) {
            console.log("è‡ªåŠ¨æäº¤é¢è¯•è€…è¯­éŸ³:", transcript);
            submitMessage(transcript);
          }
        }
      }, 1800); // 1.8ç§’å»¶è¿Ÿï¼Œå¯æ ¹æ®éœ€è¦è°ƒæ•´
    }

    return () => {
      if (autoSubmitTimerRef.current) {
        clearTimeout(autoSubmitTimerRef.current);
        autoSubmitTimerRef.current = null;
      }
    };
  }, [
    transcript,
    voiceprintEnabled,
    isInterviewer,
    submitMessage,
    resetTranscript,
    isAutoSubmit,
  ]);

  // å¼€å§‹é¢è¯•çš„å¤„ç†å‡½æ•°
  const startInterview = () => {
    setIsStarted(true);
    resetTranscript();
  };

  const stopRecognition = () => {
    try {
      // å…ˆåœæ­¢è¯­éŸ³è¯†åˆ« - ä½¿ç”¨æ›´å¼ºåˆ¶çš„æ–¹æ³•
      SpeechRecognition.abortListening();
      setTimeout(() => {
        SpeechRecognition.stopListening();
      }, 0);

      // ç¡®ä¿åœæ­¢æ‰€æœ‰éŸ³é¢‘é‡‡é›†
      stopAudioCollection();
      // é‡è¦ï¼šç¡®ä¿æ‰€æœ‰çš„TensorFlowèµ„æºè¢«é‡Šæ”¾
      if (recognizerRef.current) {
        recognizerRef.current.clearBuffer();
        recognizerRef.current = null;
      }

      // ç¡®ä¿æ¨¡å‹æ­£ç¡®é‡Šæ”¾ï¼Œé˜²æ­¢å†…å­˜æ³„æ¼
      if (modelRef.current) {
        try {
          if (!(modelRef.current as any).__disposed) {
            modelRef.current.dispose();
            (modelRef.current as any).__disposed = true;
            console.log("æ¨¡å‹å·²å®‰å…¨é”€æ¯");
          }
        } catch (e) {
          console.error("æ¨¡å‹é”€æ¯å‡ºé”™:", e);
        }
        modelRef.current = null;
      }

      // // æäº¤æœ€ç»ˆç»“æœ
      // if (transcriptRef.current && transcriptRef.current.trim() !== "") {
      //   submitMessage(transcriptRef.current);
      //   // æ·»åŠ åˆ°æ¶ˆæ¯å†å²
      //   addMessage(transcriptRef.current, isInterviewerRef.current);
      // }

      // å…³é—­overlay
      setVisible(false);

      // ç¡®ä¿æµè§ˆå™¨å›æ”¶æ‰€æœ‰åª’ä½“èµ„æº
      setTimeout(() => {
        if (audioStreamRef.current) {
          const tracks = audioStreamRef.current.getTracks();
          tracks.forEach((track) => {
            if (track.readyState === "live") {
              track.stop();
              console.log("å¼ºåˆ¶åœæ­¢é—ç•™éŸ³é¢‘è½¨é“");
            }
          });
          audioStreamRef.current = null;
        }
        onClose();
      }, 100);
    } catch (error) {
      console.error("åœæ­¢è¯­éŸ³è¯†åˆ«å¤±è´¥:", error);
    }
  };

  // æ·»åŠ æš‚åœ/æ¢å¤åŠŸèƒ½
  const togglePauseCommit = () => {
    if (!isPaused) {
      // ä½¿ç”¨æ›´å¼ºåˆ¶çš„ä¸­æ–­æ–¹å¼
      SpeechRecognition.abortListening();
      // ç„¶åå†è°ƒç”¨æ­£å¸¸çš„åœæ­¢æ–¹æ³•ç¡®ä¿å®Œå…¨åœæ­¢
      setTimeout(() => {
        SpeechRecognition.stopListening();
      }, 0);

      // æš‚åœéŸ³é¢‘é‡‡é›†
      stopAudioCollection();

      // // æ–°å¢ï¼šæ£€æŸ¥å¹¶æäº¤å½“å‰å·²è¯†åˆ«çš„æ–‡å­—
      // if (transcriptRef.current && transcriptRef.current.trim() !== "") {
      //   console.log("æš‚åœæ—¶æäº¤è¯†åˆ«å†…å®¹:", transcriptRef.current);
      //   submitMessage(transcriptRef.current);
      //   // æ·»åŠ åˆ°æ¶ˆæ¯å†å²
      //   addMessage(transcriptRef.current, isInterviewerRef.current);
      //   // æäº¤åæ¸…ç©ºå†…å®¹

      // }
      resetTranscript();
    } else {
      // å…ˆç¡®ä¿åœæ­¢å½“å‰å¯èƒ½å­˜åœ¨çš„ç›‘å¬
      SpeechRecognition.abortListening();
      // çŸ­æš‚å»¶è¿Ÿåé‡æ–°å¯åŠ¨ç›‘å¬
      setTimeout(() => {
        SpeechRecognition.startListening({
          continuous: true,
          language: recognitionLanguage,
        });
        // é‡ç½®æ–‡æœ¬
        resetTranscript();
      }, 0);

      // é‡æ–°å¼€å§‹éŸ³é¢‘é‡‡é›†
      if (voiceprintEnabled) {
        startAudioCollection();
      }
    }
    setIsPaused(!isPaused);
  };

  // æ·»åŠ æ‹–åŠ¨ç›¸å…³çš„äº‹ä»¶å¤„ç†å‡½æ•°
  const handleDragStart = (e: React.MouseEvent) => {
    setIsDragging(() => {
      isDraggingRef.current = true;
      return true;
    });
    dragStartXRef.current = e.clientX;
    initialWidthRef.current = parseInt(width);
    document.addEventListener("mousemove", handleDragMove);
    document.addEventListener("mouseup", handleDragEnd);
  };

  const handleDragMove = (e: MouseEvent) => {
    if (isDraggingRef.current) {
      const deltaX = e.clientX - dragStartXRef.current;
      const newWidth = Math.max(
        15,
        Math.min(
          80,
          initialWidthRef.current - (deltaX / window.innerWidth) * 100,
        ),
      );
      console.log(`mouse have moved  Width:${newWidth}vw`);
      setWidth(`${newWidth}vw`);
    }
  };

  const handleDragEnd = () => {
    setIsDragging(() => {
      isDraggingRef.current = false;
      return false;
    });
    document.removeEventListener("mousemove", handleDragMove);
    document.removeEventListener("mouseup", handleDragEnd);
  };

  // ç»„ä»¶å¸è½½æ—¶æ¸…ç†äº‹ä»¶ç›‘å¬å™¨
  useEffect(() => {
    return () => {
      document.removeEventListener("mousemove", handleDragMove);
      document.removeEventListener("mouseup", handleDragEnd);

      // æ¸…ç†å…¶ä»–èµ„æº
      if (autoSubmitTimerRef.current) {
        clearTimeout(autoSubmitTimerRef.current);
      }

      stopAudioCollection();
    };
  }, []);

  if (!visible) {
    return null;
  }

  return (
    <div
      className={`interview-overlay ${isDragging ? "dragging" : ""} ${
        isInterviewerRef.current && voiceprintEnabled ? "interviewer-mode" : ""
      }`}
      style={{ width }}
    >
      {/* æ·»åŠ å·¦ä¾§æ‹–åŠ¨æ¡ */}
      <div className="drag-handle" onMouseDown={handleDragStart} />

      <div className="content-container">
        {!isStarted ? (
          // é¢è¯•å‡†å¤‡ç»„ä»¶
          <InterviewPreparation
            onStart={startInterview}
            voiceprintEnabled={voiceprintEnabled}
            setVoiceprintEnabled={setVoiceprintEnabled}
          />
        ) : (
          // å·²å¼€å§‹é¢è¯•æ—¶æ˜¾ç¤ºé¢è¯•ç•Œé¢
          <>
            {/* è¯­éŸ³è¯†åˆ«çŠ¶æ€æŒ‡ç¤ºå™¨ */}
            <div className="status-indicator">
              <div
                className={`indicator-dot ${
                  listening ? "listening" : "not-listening"
                }`}
              />
              <span className="status-text">
                {listening ? "æ­£åœ¨ç›‘å¬..." : isPaused ? "å·²æš‚åœ" : "æœªç›‘å¬"}
              </span>

              {/* æ·»åŠ å¯ç‚¹å‡»æç¤ºæ°”æ³¡ */}
              {showTooltip && (
                <div className="clickable-tooltip">
                  <div className="tooltip-content">å•å‡»ä¸‹é¢çš„æ¶ˆæ¯è·å–å›ç­”</div>
                  <button
                    className="tooltip-close-btn"
                    onClick={() => setShowTooltip(false)}
                  >
                    âœ•
                  </button>
                </div>
              )}

              {/* æ·»åŠ å£°çº¹è¯†åˆ«çŠ¶æ€æ˜¾ç¤º */}
              {voiceprintEnabled && (
                <div className="voiceprint-status">
                  <span
                    className={`identity-indicator ${
                      isInterviewerRef.current ? "interviewer" : "interviewee"
                    }`}
                  >
                    {isInterviewerRef.current ? "é¢è¯•å®˜" : "é¢è¯•è€…"}
                  </span>
                  {voiceMatchScoreRef.current > 0 && (
                    <span className="match-score">
                      ç›¸ä¼¼åº¦: {(voiceMatchScoreRef.current * 100).toFixed(1)}%
                    </span>
                  )}
                </div>
              )}
            </div>

            {/* é”™è¯¯æç¤º */}
            {(!browserSupportsSpeechRecognition || !isMicrophoneAvailable) && (
              <div className="error-message">
                {!browserSupportsSpeechRecognition
                  ? "æ‚¨çš„æµè§ˆå™¨ä¸æ”¯æŒè¯­éŸ³è¯†åˆ«åŠŸèƒ½,è¯·ä½¿ç”¨Chromeæµè§ˆå™¨"
                  : "æ— æ³•è®¿é—®éº¦å…‹é£ï¼Œè¯·æ£€æŸ¥éº¦å…‹é£æƒé™"}
              </div>
            )}

            {/* æ¶ˆæ¯å†å²è®°å½•åŒºåŸŸ */}
            <div className="messages-container">
              {messages.map((message) => (
                <div
                  key={message.id}
                  className={`message ${
                    message.isInterviewer
                      ? "interviewer-message"
                      : "interviewee-message"
                  }`}
                  onClick={() => handleMessageClick(message.text)}
                >
                  {message.text}
                </div>
              ))}
              <div ref={messagesEndRef} />
            </div>

            {/* å½“å‰è¯†åˆ«æ–‡æœ¬æ˜¾ç¤ºåŒºåŸŸ */}
            {transcript && transcript.trim() !== "" && (
              <div
                className={`transcript-display ${
                  voiceprintEnabled && isInterviewerRef.current
                    ? "interviewer-text"
                    : "interviewee-text"
                }`}
              >
                <div className="transcript-label">å½“å‰è¯†åˆ«:</div>
                {transcript}
              </div>
            )}

            {/* æŒ‰é’®åŒºåŸŸ */}
            <div className="button-container">
              {/* æ·»åŠ è‡ªåŠ¨æäº¤çš„å¼€å…³ */}
              <div className="setting-item">
                <div className="setting-label">è‡ªåŠ¨æäº¤ï¼š</div>
                <div className="setting-control">
                  <label className="switch">
                    <input
                      type="checkbox"
                      checked={isAutoSubmit}
                      onChange={
                        voiceprintEnabled
                          ? () => setIsAutoSubmit(!isAutoSubmit)
                          : () => {}
                      }
                    />
                    <span className="slider"></span>
                  </label>
                  <span className="setting-status">
                    {voiceprintEnabled
                      ? "å¯å¯ç”¨"
                      : "å£°çº¹æœªå¯ç”¨,è¯·å…ˆæ‰“å¼€å£°çº¹è¯†åˆ«"}
                  </span>
                </div>
              </div>

              {/* æš‚åœæ¢å¤æŒ‰é’® */}
              <button
                onClick={togglePauseCommit}
                className={`button pause-button ${isPaused ? "paused" : ""}`}
              >
                <span>{isPaused ? "â–¶ï¸ æ¢å¤ç›‘å¬" : "â¸ï¸ æš‚åœç›‘å¬"}</span>
              </button>

              <button onClick={stopRecognition} className="button stop-button">
                <StopIcon />
                <span>ç»“æŸé¢è¯•</span>
              </button>

              <button onClick={resetTranscript} className="button clear-button">
                <span>ğŸ—‘ï¸ æ¸…ç©º</span>
              </button>
            </div>
          </>
        )}
      </div>

      {/* æ·»åŠ Toasterç»„ä»¶ç”¨äºæ˜¾ç¤ºToasté€šçŸ¥ */}
      <Toaster
        position="top-center"
        toastOptions={{
          duration: 3000,
          style: {
            borderRadius: "8px",
            background: "#fff",
            color: "#333",
            boxShadow: "0 3px 10px rgba(0, 0, 0, 0.1)",
          },
        }}
      />
    </div>
  );
};
